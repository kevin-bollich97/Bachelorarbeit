\chapter{Richtlinien für das CFT Portale}
\label{richtlinien}

In diesem Kapitel werden Richtlinien für das CFT Portale definiert. 
Mithilfe dieser Richtlinien soll es den Entwicklern ermöglicht werden, ihren Logging-Mechanismus einheitlich und strukturiert zu entwickeln. 
Derzeit wird das Logging inkonsequent und ohne Vorgaben durchgeführt. 
Dadurch ist es nicht möglich, mit den entstandenen Logdateien effektiv Bugtracking zu betreiben und schnellstmöglich einen Fehler ausfindig zu machen. \\
Damit ein effektives Logging ermöglicht werden kann, werden Richtlinien in vielen Bereichen des Loggings definiert. 
Darunter fallen die Informationen, die geloggt werden sollen, außerdem auch Richtlinien zu den zu nutzenden Tools. 

\section{Was soll geloggt werden?}
% Welche Informationen sollen geloggt werden? 
% Was ist wirklich wichtig? 
% Id des Prozesses...

Eine große Herausforderung beim Erstellen von Logs, ist die Zusammenfassung der relevantesten Informationen eines ausgelösten Events.
Dabei ist es wichtig, zu entscheiden, welche Informationen wirklich relevant sind. 
Grob kann gesagt werden, dass zum Beispiel der Timestamp und die geworfene Fehlermeldung wichtig sind.  
Jedoch fehlen noch wesentliche Informationen, die für das Verständnis des Logs essenziell sind.  \\
In Kapitel \ref{welcheInformationen} wurden Informationen vorgestellt, die geloggt werden sollten. 
Dies war die Theorie, aber wie kann die Theorie in Praxis überführt werden? \\

In der Studie \citetitle*{heCharacterizingNaturalLanguage2018} \cite{heCharacterizingNaturalLanguage2018} wurden 10 Java und 7 C\# Projekte nach ihren Logging-Statements untersucht.
All die untersuchten Projekte sind Open-Source.
In der Studie sind die Logarten in die Kategorien \textit{Program Operation, Error Message} und \textit{Semantic Description} unterteilt.
Jede der Kategorien beinhaltet weitere Unterkategorien, die spezieller angewandt werden. 
Im weiteren Verlauf werden die einzelnen Kategorien vorgestellt.

    \subsection{Program Operation}
    In der Kategorie \textit{Program Operation} beschreiben Logs den Programmcode. 
    Hier werden Informationen geloggt, die beschreiben, welche Teile des Codes gerade ablaufen. 
    Das Log-Level ist hier in der Regel \textit{Info}.
    Je nach Platzierung im Code kann die Kategorie in weitere Unterkategorien eingeteilt werden. 
    Die Unterkategorien sind dann: \textit{completed operation}, \textit{current operation} und \textit{next operation}. 

    \subsubsection{completed operation}
    \textit{Completed Operations} werden am Ende einer Funktion oder eines Anweisungsblocks geschrieben. 
    Mit denen wird das Beenden eines bestimmten Bereichs beschrieben.  

    \begin{lstlisting}[caption=completed operation \cite{heCharacterizingNaturalLanguage2018}]
/* Example 1: Completed Operation */
final lbmonitor monitorObj = lbmonitor.get(_netscalerService, nsMonitorName);
monitorObj.set_respcode(null);
lbmonitor.delete(_netscalerService, monitorObj);
s_logger.info("Successfully deleted monitor : " + nsMonitorName);
    \end{lstlisting}

    \subsubsection{current operation}
    \textit{Current operations} werden in while oder for schleifen genutzt. 
    Sie beschreiben den aktuellen Status einer bestimmten Aktion. 
    \begin{lstlisting}[caption=current operation \cite{heCharacterizingNaturalLanguage2018}]
/* Example 2: Current Operation */
while (nm.getServiceState() != STATE.STOPPED && waitCount++ != 20) {
    LOG.info("Waiting for NM to stop..");
    Thread.sleep(1000);
}
    \end{lstlisting}

    \subsubsection{next operation}
    \textit{Next operations} beschreiben, was als Nächstes im Programm passieren soll. 
    Daher werden diese Logs immer zu Beginn eines Anweisungsblocks oder einer Funktion geschrieben. 

    \begin{lstlisting}[caption=next operation \cite{heCharacterizingNaturalLanguage2018}]
/* Example 3: Next Operation */
LOG.info("Close consumer on A");
clientA.close();
    \end{lstlisting}


    \subsection{Error Message}
    Eine der wichtigsten Informationen im Logging sind die Error Messages, die während der Programmlaufzeit auftreten. 
    Daher müssen diese auch geloggt werden.
    Bei dieser Log-Kategorie werden Informationen zu den auftretenden Fehlern beschrieben.
    Die Kategorie kann in 2 weitere Unterkategorien eingeteilt werden. 
    Diese sind: \textit{exception} und \textit{value-check}.

    \subsubsection{exception}
    Exceptions werden in try-catch-Blöcken gefangen und geloggt. 
    Dabei können unterschiedliche Arten von Exceptions gefangen werden und dann auch auf eigene Art und Weise geloggt werden. 

    \begin{lstlisting}[caption=exception \cite{heCharacterizingNaturalLanguage2018}]
/* Example 4: Exception */
try {
    count = c.readAndProcess();
} catch (InterruptedException ieo) {
    LOG.info(Thread.currentThread().getName() + ": readAndProcess caught InterruptedException", ieo);
    throw ieo;
}
    \end{lstlisting}

    \subsubsection{value-check}
    Value-checks überprüfen bestimmte Werte, die erwartet werden. 
    Dies geschieht nicht in try-catch-Blöcken wie in der Unterkategorie \textit{exception}, sondern die Werte werden in If-Anweisungen überprüft und ausgewertet. 

    \begin{lstlisting}[caption=value-check \cite{heCharacterizingNaturalLanguage2018}]
/* Example 5: Value-Check */
if (jobId == null) {
    s_logger.error("Unable to get a jobId");
    return null;
}
    \end{lstlisting}

    \subsection{Semantic Description}

    Während der Wartung von Software, wird der zu wartende Code, gedebuggt. 
    Dabei werden detailliertere Informationen benötigt.
    Diese werden in der \textit{Semantic Description} beschrieben. 
    Die hier auftretenden Logs werden im Log-Level \textit{debug} geloggt. 
    In der Semantic Description Kategorie, können wieder drei weitere Unterkategorien beschrieben werden. Das sind: \textit{variable description}, \textit{function description} und \textit{branch description}.

    \subsubsection{variable description}
    In dieser Unterkategorie werden die enthaltenen Werte von Variablen geloggt. 
    So kann überprüft werden, ob die Variable den zu erwartenden Wert enthält. 

    \begin{lstlisting}[caption=Variable Description \cite{heCharacterizingNaturalLanguage2018}]
/* Example 6: Variable Description */
String messageID = (String) element.get("JMSMessageID");
LOG.debug("MessageID: {}", messageID);
    \end{lstlisting}

    \subsubsection{function description}
    In \textit{function description} werden Funktionen und ihre Attribute beschrieben.
    Dabei werden die übergebenen Werte der Funktion angezeigt, um mögliche Fehlerquellen zu erkennen.
    Dieses Log wird immer zu Beginn einer Funktion ausgegeben. 

    \begin{lstlisting}[caption=funtion description \cite{heCharacterizingNaturalLanguage2018}]
/* Example 7: Function Description */
public void forget(Xid xid) throws XAException {
    LOG.debug("Forget: {}", xid);
    ...
}
    \end{lstlisting}

    \subsubsection{branch description}
    \textit{Branch description} beschreibt einen möglichen Pfad im Quellcode. 
    Dies wird in If-Else Anweisungen genutzt. 
    So kann der ausgeführte Pfad einer Anweisung erkannt werden. 

    \begin{lstlisting}[caption=branch description \cite{heCharacterizingNaturalLanguage2018}]
/* Example 8: Branch Description */
if (blobItem instanceof CloudBlockBlobWrapper || blobItem instanceof CloudPageBlobWrapper) {
    LOG.debug("Found blob as a directory-using this file under it to infer its properties {}", blobItem.getUri());
    ...
}
    \end{lstlisting}

    \subsection{Wie ist ein Log aufgebaut?}
    Während der Fehlersuche müssen viele Logdateien durchsucht werden. 
    Dabei sollten die einzelnen Log-Einträge einen ähnlichen Aufbau haben. 
    So kann die Suche nach Fehlern gezielter und dadurch auch effizienter ablaufen. \\
    Die vorgestellten Kategorien zeigen, dass Logs entweder nur aus statischem Text bestehen oder eine dynamische Komponente mit einbringen. 
    Der dynamische Teil enthält dabei Variablen oder Objekte. 
    Durch diesen dynamischen Teil kann ein Fehler in bestimmten Fällen besser erkannt werden. 
    Denn der dynamische Teil bringt Werte aus dem Quellcode in die Logs. 
    Zum Beispiel kann beim Aufruf einer Funktion der Fehler auftreten, dass der übergebene Wert \textit{null} ist. 
    Auf den ersten Blick ist es nicht klar, dass der Wert \textit{null} ist.
    Wenn aber im Log-Level Debug, die für die Funktion übergebenen Parameter im Log angezeigt werden, kann dieser Fehler schnell erkannt werden. 
    Damit die Fehlersuche effizienter und besser erfolgen kann, wird das strukturierte Logging verwendet. Dies wird jedoch in Kapitel \ref{kap:dasRichtigeLogLevel} erklärt. \\

    Damit die relevanten Informationen in Logfiles zu erkennen sind, müssen diese eine klare Struktur aufzeigen. 
    Die Reihenfolge eines Log-Eintrags sollte diesem Aufbau entsprechen, so dass alle relevanten Informationen in einer klaren Struktur vorliegen.
    \begin{itemize}
        \item Timestamp 
        \item LogLevel
        \item Correlation ID 
        \item System Komponente
        \item String, um den Fehler oder das Ereignis zu beschreiben
    \end{itemize}

    Die Correlation ID ermöglicht es, einen Prozess von einem Benutzer von Beginn der Session bis zum Ende zu verfolgen. 
    Im Falle des CFT Portale ist es wichtig, die Correlation ID im Log mitanzugeben, weil ein Nutzer auf mehreren VMs aktiv sein kann und so keine eindeutige Zuordnung mehr möglich ist. 
    Damit ist die redundante Serververteilung gemeint, diese wurde in Kapitel \ref{redundanteSerververteilung} beschrieben. 
    Die Correlation ID wird dabei zu Beginn einer Session erstellt. 
    Während der Session wird diese Correlation ID dann in allen Logs angegeben.

% Ein Log sollte enthalten: 
% \begin{itemize}
%     \item Das LogLevel
%     \item einen Timestamp
%     \item Aus welcher Komponente kam der Log (Server, Client, Klasse?)
%     \item der, der das Event ausgelöst hat (Username oder die UserID oder irgendwas verschlüsseltes und auch eine ProzessID)
%     \item geworfene Exception, oder eine eigens geschriebene Meldung (z.B. Connection zur Datenbank funktioniert nicht)
%     \item Der Name der Methode oder des generellen Prozesses der ausgeführt wurde (Dies kann der Login prozess sein oder ähnliches)
% \end{itemize}

\section{Wie soll geloggt werden?}
% Strukturiertes Logging, Welches Framework, Welche Log Level sollen genutzt werden
% Logstash und Splunk angucken cite(KabinnaANdShang)
\label{kap:dasRichtigeLogLevel}
Eine wichtige Entscheidung beim Loggen ist die Entscheidung zum genutzten Logging-Framework. 
Jedes Framework bringt seine eigenen Vorteile mit. 
Das CFT Portale setzt derzeit auf das Log4Net Framework. 
Da dieses Framework, wie in Kapitel \ref{log4net} schon beschrieben wurde, nicht mehr aktiv gewartet wird, muss sich das Team umorientieren. \\

Als Alternativen zu Log4Net können NLog oder Serilog verwendet werden. 
NLog wurde 2006 veröffentlicht. 
Serilog ist erst 2013 veröffentlicht worden.
Beide Frameworks bieten strukturiertes Logging an.
Ein Vorteil von NLog ist die \textit{on-the-fly} Konfiguration. 
Ziele der Logs und auch das auszugebene Log-Level können verändert werden ohne das die Software neu gestartet werden muss. 
In Serilog ist dies nicht so einfach möglich.
Die Community und die vorhandenen Tutorials sind für Serilog nicht so vertreten wie bei NLog. 
Der Grund dafür ist das Alter der beiden Frameworks. 
NLog ist älter als Serilog und besitzt daher eine größere Community.
Wegen der Community und der \textit{on-the-fly} Konfiguration ist die hier festgelegte Richtlinie, die Nutzung von NLog. \\

Der derzeit verwendete Logging-Code des CFT Portale ist in Quellcode \ref{logCodeCFT} abgebildet.
Die daraus resultierende Ausgabe ist in Quellcode \ref{logAusgabeCFT} zu sehen sehen. \\
Ein Unterschied zwischen der derzeitigen und der angestrebten Methode wird erst erkennbar, wenn die Daten in einer Datenbank gespeichert werden sollen. 
Der Grund dafür ist, dass bei der Ausgabe auf die Konsole, das Log in beiden Fällen identisch aussieht. 
Jedoch ist die Struktur im Hintergrund anders aufgebaut. 
Die aktuelle Methode speichert die Log-Message als reinen Text ab. 
Dies verhindert die Weiterverarbeitung der Nachricht. 
Wobei hingegen das strukturierte Logging, die Inhalte der Nachricht als z.B. JSON abspeichern kann. 
So können die einzelnen Attribute (wie z.B. Timestamp) abgefragt werden. 
Für die Ausgabe auf der Konsole wird der Inhalt der JSON-Datei in reinen Text umgewandelt.
Im NLog Framework kann in der Konfiguration selber entschieden werden, welches Format verwendet werden soll. \\

\begin{lstlisting}[caption=Logging code vom CFT Portale, label=logCodeCFT]    
    log.Info($"[Controller] {logMetadata.ToString()}");
\end{lstlisting}

\begin{lstlisting}[caption=Logausgabe vom CFT Portale, label=logAusgabeCFT]
2020-07-16 01:09:22,017 [17] INFO  Username 8001421c-0001-c700-b63f-84710c7967bb  : [Controller] "logMetadata.ToString()"
\end{lstlisting}

Der Quellcode \ref{logCodeStrukturiert} loggt das gleiche wie der Quellcode \ref{logCodeCFT}.
Jedoch wird in Quellcode \ref{logCodeStrukturiert} strukturiert geloggt. 
Wie im letzten Absatz erwähnt, werden beim strukturierten Logging Teile des Logs ins Variablen aufgeteilt. 
Im Beispiel des Quellcodes \ref{logCodeStrukturiert}, werden zwei Variablen erzeugt. 
Zu erst die Systemkomponente und anschließend die Metadaten. 
Der Name der Variable, ist in geschweiften Klammern geschrieben (\{system\}, \{logmetadata\}).
Nach dem Ende des Strings folgen die einzelnen Werte der Variablen. 
Im Beispiel sind die Werte \textit{Controller} und der String aus der Methode \textit{logMetadata.ToString()}.
Dabei gilt die Reihenfolge der im String erwähnten Variablen, der mit Komma separierten Werte. 
Diese Ausgliederung ermöglicht effizienteres Durchsuchen der Logs. 
Durch das Setzen der Variablen, werden in dem entstehenden Logfile, diese Variablen mit eingefügt. 
So kann das Logfile nach diesen Variablen zusätzlich gefiltert und durchsucht werden. 
Das zu dem Quellcode \ref{logCodeStrukturiert} passende JSON-Format, ist in Quellcode \ref{logAusgabeStrukturiert} zu sehen. \\


Das strukturierte Logging soll in Zukunft der Standard des CFT Portale sein. 
Daher wird das strukturierte Logging als Richtlinie definiert.


\begin{lstlisting}[caption=Strukturierter Logging Code, label=logCodeStrukturiert]
    log.info("[{system}] {logmetadata}", "Controller", logMetadata.ToString());
\end{lstlisting}

\begin{lstlisting}[caption=Speicherung des strukturierten Loggings, label=logAusgabeStrukturiert]
    {
        "timestamp": "2020-07-16 01:09:22,017",
        "system": "Controller",
        "logmetadata": "String von Funktion logMetadata.ToString()"
        "message": "[Controller] logMetadata.ToString()"
    }
\end{lstlisting}


In der Studie \citetitle*{liWhichLogLevel2016} wurden vier Projekte untersucht. 
Hier wurde das Verhalten von Entwicklern im Zusammenhang mit den unterschiedlichen Log-Leveln betrachtet. 
Die Kernfragen sind dabei, welche Log-Level werden benutzt, wie häufig werden diese verwendet und auch, wo werden diese genutzt. \\
In dem Diagramm \ref{fig:nutzunglogLevel} werden die Anzahl an geschriebenen Logs in Abhängigkeit der Log-Level dargestellt.
Dabei zeigt die x-Achse alle sechs relevanten Log-Level.
Auf der y-Achse wird die Anzahl an geschrieben Logs und die Zugehörigkeit zu einem Projekt angegeben.
Um die Menge besser zu veranschaulichen, werden die Werte nicht nur in Anzahl, sondern auch der im Projekt genutzte Anteil in Prozent angegeben. 
Hier fällt auf, dass die beiden extremen Log-Level, \textit{trace} und \textit{fatal} am wenigsten genutzt werden. 
Daraus kann geschlossen werden, dass die beiden Log-Level in der Praxis keine starke Relevanz haben. 
Der Grund dafür ist der ähnliche Nutzen zwischen \textit{trace} und \textit{debug} und zwischen \textit{error} und \textit{fatal}. 
Bei \textit{trace} und \textit{debug} werden triviale Informationen geloggt, die nur für die Entwicklung relevant sind. 
Bei \textit{error} und \textit{fatal} werden Fehler geloggt, die in den meisten Fällen einen Abbruch verursachen können. 
Dabei ist es schwierig im Code vorauszusagen, ob der Fehler jetzt \textit{fatal} oder \glqq nur\grqq{} ein \textit{error} ist. 
Daher wird in der Regel \textit{error} genutzt.
Als Konsequenz dieser Projektarbeit, definiert die Richtlinie für das CFT, dass \textit{trace} gar nicht genutzt werden soll. 
Alle Informationen, die für die Entwicklung notwendig sind, sollten in \textit{debug} geloggt werden. 
Das Log-Level \textit{fatal} soll hingegen weiterhin genutzt werden.
Sollte ein spezieller Fall klar sein, in dem ein fataler Fehler auftreten kann, der ein komplett Absturz der Anwendung verursachen würde, dann soll das Log-Level genutzt werden. \cite{liWhichLogLevel2016} 

\begin{figure}[H]
    \centering
    \includegraphics[width=1\textwidth]{pictures/NutzungLogLevel.PNG}
    \caption{Log-Level Nutzung je Projekt \cite{liWhichLogLevel2016}}
    \label{fig:nutzunglogLevel}
\end{figure}

Im Quellcode werden Logging-Statements an verschiedenen Stellen verwendet. 
Je nach Platzierung, werden unterschiedliche Log-Level genutzt. \\
Die sieben relevantesten und am häufigsten genutzten Code-Blöcke sind: 
\begin{itemize}
    \item try
    \item catch
    \item for
    \item while
    \item methods
    \item if-else
    \item switch-case-default
\end{itemize}
Jeder dieser Code-Blöcke sind in der Entwicklung von Software essentiell wichtig. 
In ihnen werden logische Operationen durchgeführt und sorgen für einen reibungslosen Ablauf der Software.
Sie fangen Fehler ab und sorgen dafür, dass die Software nicht abstürzt.
In den unterschiedlichen Code-Blöcken können Logging-Statements gesetzt werden. 
Es können aber grobe Richtlinien bestimmt werden, die einen Überblick geben, welche Informationen in bestimmten Code-Blöcken relevant sind. 
Je nach Anwendungsfall kann dies in der Praxis von den Richtlinien abweichen, jedoch sind das Ausnahmefälle. \\
Ein Logging-Statement, dass in einem \textit{try-Block} geschrieben wurde, enthält in der Regel Laufzeitinformationen. 
Denn diese werden bei normalem Ablauf der Software ausgeführt. 
Daher sollen in \textit{try-Blöcken} die Log-Level \textit{info} und \textit{debug} genutzt werden.
Hier können dann Informationen über den Verlauf der Software dokumentiert werden oder auch entwicklungsrelevante Informationen. \\
Die Logging-Statements in \textit{catch-Blöcken} werden nur bei fehlerhaftem Verlauf der Software ausgeführt. 
Daher wird in \textit{catch-Blöcken} mit den Log-Leveln \textit{warn}, \textit{error} und \textit{fatal} geloggt. 
Bei \textit{catch-Blöcken} muss darauf geachtet werden, dass nicht jeder Fehler gleich ein \textit{error} ist. 
Das Log-Level \textit{warn} kann in vielen Fällen die Situation in \textit{catch-Blöcken} besser beschreiben. \\
In den Schleifen \textit{for} und \textit{while} können die Anweisungen, die im Schleifenrumpf definiert sind, in einer Vielzahl an Iterationen aufgerufen werden. 
Daher ist es bei Schleifen ratsam auf ein ausführliches Level zu setzen.
Das wären dann entweder \textit{debug} oder \textit{info}.
Da jedoch die Informationen in Schleifen eher in der Entwicklung relevant sind, sollte das Log-Level \textit{debug} bevorzugt werden. \\
In den Code-Blöcken \textit{if-else} und \textit{switch-case-default} können alle Log-Level genutzt werden. 
Hier muss auf den Anwendungskontext geachtet werden:
Wenn bei \textit{if-Anweisungen} nach Fehlern gesucht wird, dann sollte \textit{Error} genutzt werden. 
Sollte jedoch eine triviale Abfragen durchgeführt werden, um dann eine Anweisung durchzuführen. 
Wäre die Nutzung eines ausführlicheren Log-Levels ratsam. 
Das wären dann \textit{info} und \textit{debug}.
Hier müssen die Entwickler also eigene Entscheidung darüber treffen, welches Log-Level sie am besten nutzen wollen. \cite{liWhichLogLevel2016}


\section{Wo sollen die Logs gespeichert werden?}

%https://sematext.com/blog/best-log-management-tools/
%https://sematext.com/blog/logstash-alternatives/
%https://www.comparitech.com/net-admin/log-management-tools/

% Zentralisiertes Logging? 
% In eine Datei oder Datenbank? 
% Gibt es noch andere Möglichkeiten? 
Durch die Speicherung aller entstehenden Logs der unterschiedlichen Anwendungen an einem zentralen Ort wird es ermöglicht, dass der Verlauf eines Prozesses besser erkannt werden kann und schneller mögliche Fehlerursachen gefunden werden. 
Weil die Daten alle an einer Stelle einsehbar sind, fällt das Durchsuchen der einzeln erstellten Logs auf den Servern weg. \\
Um diese zentrale Speicherung der Logs zu ermöglichen, können viele Tools genutzt werden. 
Für das CFT Portale wird der Elastic Stack genutzt. 
Der Elastic Stack soll genutzt werden, weil im Team vor der Projektarbeit schon mit dem gearbeitet wurde.
Damals wurde versucht eine zentralisierte Lösung mit dem Elastic Stack zu entwickeln. 
Dies wurde jedoch durch Zeitmangel vernachlässigt und abgebrochen. 
Im Rahmen der anstehenden Bachelorarbeit sollen weitere Tools evaluiert werden, um zu prüfen, ob der Elastic Stack die Beste Lösung für das Team ist. \\
Der Elastic Stack kümmert sich um das Speichern, Durchsuchen und Parsen der Logs. 
Er besteht aus vier Produkten: \textit{Beats}, \textit{Logstash}, \textit{Elasticsearch} und \textit{Kibana}. \cite{elasticELKStackElasticsearch}

Alle vier Produkte des Elastic-Stack sind Open-Source Software. 
Sie ermöglichen gemeinsam einen effektiven Umgang mit erstellten Logs. 
Die Produkte unterscheiden sich in ihren Aufgaben. \\

Damit der nächste Prozess und die vorgestellten Tools besser verstanden werden können, wurde die Abbildung, die den zentralisierten Prozess aus Kapitel \ref{kap:zentralLogging} beschreibt, noch einmal mit eingebunden. 
Die einzelnen Tools, die mit dem Elastic Stack hinzukommen, beschreiben jeweils eine eigene Komponente aus der Abbildung. 
Die Komponenten \textit{Logs producer} und \textit{Log file} aus der Abbildung werden in diesem Kapitel nicht erklärt, da sie in Kapitel \ref{kap:zentralLogging} definiert wurden. \\

Die Beats Komponente aus dem Elastic Stack kümmert sich um das Senden der Logfiles an die dafür vorgesehenen Ziele.
Dieser wird auf den Log-produzierenden Maschinen installiert. 
Die Komponente aus der Abbildung \ref{fig:centralLoggingKap4} ist die \textit{Log transmission}.
Die Daten können direkt zu Elasticsearch gesendet werden oder, wenn sie noch geparst oder transformiert werden müssen, müssen sie zu Logstash, dem \textit{Log collector} gesendet werden.  
Durch Beats können Daten von hunderten oder tausenden Geräten gesendet werden. 
Für jeden Anwendungsfall gibt es eigene Beats-Komponenten.
Derzeit gibt es sieben offizielle. 
Diese sind: \textit{Filebeat, Metricbeat, Packetbeat, Winlogbeat, Auditbeat, Heartbeat und Functionbeat} \cite{objectrocketWhatAreElasticsearch} \\ 

Logstash wird zur Erfassung, Verarbeitung, Transformation und Weiterleitung von Daten in bestimmte Datenziele benötigt.
Die Komponente ist hier der \textit{Log collector}.
Dabei können die Daten von den unterschiedlichsten Quellen herangezogen werden. 
Die Verarbeitung der Daten erfolgt in Logstash mit so genannten Pipelines. 
In jeder Pipeline werden \textit{input-plug-ins}, \textit{output-plug-ins} und \textit{Filter} definiert. 
Die input-plug-ins sind für das Einlesen aus verschiedenen Datenquellen verantwortlich. 
Die einzulesenden Daten können in vielen unterschiedlichen Formaten gelesen werden. 
In einem kontinuierlichen Stream werden die Daten dann an den Filter gesendet.
Der Filter sorgt dann dafür, dass die Daten in eine gewünschte Form umgewandelt werden. 
Hier ist es möglich, unstrukturierten Daten Struktur zu verleihen. 
Der Abschluss der Pipeline sind dann die output-plug-ins. 
Diese vermitteln die von den Filtern umstrukturierten Daten an ein Datenziel. 
Dabei ist ein Datenziel Elasticsearch.
Die Weitergabe an andere Datenziele ist jedoch auch möglich, z.B. eine Relationale Datenbank. \cite{luberWasIstLogstash} \\


\begin{figure}[H]
    \includegraphics[width=1\textwidth]{pictures/CentralLoggingProzess.PNG}   
    \caption{Prozess zentrales Logging \cite{vainioImplementationCentralizedLogging2018}}
    \label{fig:centralLoggingKap4}
\end{figure} 

Elasticsearch kümmert sich um die Suche und die Speicherung der Logs.
Die dazugehörige Komponente aus der Abbildung ist \textit{Log storage}. 
Die Daten werden in der Datenstruktur des invertierenden Index gespeichert. 
Der Index wird vor der Suche durch Indizierung der Rohdaten erstellt. 
Dabei ist der Index in Typen und Dokumenten unterteilt. 
Die Typen verhalten sich in der Suche ähnlich wie Tabellen. 
In den Dokumenten werden die eigentlichen Daten gespeichert die durchsucht werden sollen. \cite{luberWasIstElasticsearch2020} \\

Als Analyse- und Visualisierungsplattform wird im Elastic-Stack Kibana genutzt. 
Die dazugehörige Komponente ist \textit{Log analysis}.
Das Tool ermöglicht die Visualisierung der vom Elasticsearch indizierten Daten. 
Es können unterschiedliche Diagramme zur Visualisierung genutzt werden. 
Als Basis können die klassischen Visualisierungsmethoden wie Histogramme, Graphen, Liniendiagramme, Kreisdiagramme und Ringdiagramme genutzt werden.
Diese Visualisierungsmethoden können bei der Analyse der Software und deren Verhalten behilflich sein.  \cite{luberWasIstKibana} \\ 


In Abbildung \ref{fig:elkStack} ist der geplante Aufbau der Logging-Architektur zu sehen. 
Die Abbildung zeigt auf der linken Seite die schon vorhandenen Provider-hosted Apps. 
Jede App ist auf einer eigenen Virtuellen Maschine installiert und besitzt eine Backend- und eine Frontend-Applikation. 
Beide erzeugen jeweils eigene Logdateien. 
Die Dokumente müssen dann an die vorgesehene Stelle geschickt werden. 
Dafür wird die Beats Komponente aus dem Elastic Stack auf den jeweiligen Maschinen der Provider-hosted Apps installiert. 
Beats sammelt alle Logdateien und sendet diese dann an Logstash. 
Die restlichen drei Elastic Stack Komponenten sind auf einer eigenen VM installiert. 
Wenn die Dateien in Logstash ankommen, werden diese nach Bedarf geparst und dann zur Speicherung an Elasticsearch gesendet. 
Um die gespeicherten Logs zu durchsuchen und zu visualisieren werden die Daten von Kibana gepullt. \\

Durch die Speicherung der Logs an einer zentralen Stelle werden für das CFT Portale neue Möglichkeiten geschaffen. 
Das Team kann über eine Plattform (Kibana) auf alle erstellten Logs der unterschiedlichen Apps zugreifen und so effizientes Bugtracking durchführen. 
Die unkomplizierte und anschauliche Darstellung von Kibana ermöglicht außerdem das schnelle Einsteigen in das Tool. \\
Mithilfe der zentralisierten Speicherung können die folgenden Probleme gelöst werden. 
Das Durchsuchen der einzelnen Maschinen wird nicht mehr notwendig sein.Auch die Suche nach einzelnen Prozessen, die auf unterschiedlichen Maschinen abliefen, wird nicht mehr stattfinden. 
Ein Prozess kann durch einfaches Filtern in Kibana erkannt werden.
Das zentralisierte Logging ist ein Teil der in der Projektarbeit erstellten Richtlinien.


\begin{figure}[H]
    \includegraphics[width=1\textwidth]{pictures/ELK_Stack.png}   
    \caption{Zentralisiertes Logging mit dem ELK-Stack }
    \label{fig:elkStack}
\end{figure} 


\section{Wie lange sollten Logdateien gespeichert werden?}

Eine Forschungsfrage, die im Rahmen dieser Projektarbeit beantwortet werden soll, ist die Frage der Dauer der Speicherung. 
Da die vom Team erhobenen Logdaten bei Erhalt anonymisiert werden, gibt die Datenschutzerklärung keine Maximaldauer an. \cite{RichtlinieEU20162016} \\
Derzeit werden die Logdateien, wie im Anhang \ref{cftPortaleDatenschutzerklaerung} beschrieben, jedoch nach 14 Tagen gelöscht. 
Der Grund dafür ist der Nutzen für die Logdateien. 
Denn nach 14 Tagen werden die Logs nicht mehr gebraucht. 
Die Mindestdauer der Speicherung beläuft sich auf mindestens sieben Tage. 
Denn es kann passieren, dass die Weihnachtsfeiertage so gelegt sind, dass die Benachrichtigung eines Fehler erst nach sieben Tagen erfolgen kann. 
Daher wird hier festgelegt, dass die Logs mindestens sieben Tage gespeichert werden sollen. \\
Das Team legt die Dauer der Speicherung jedoch nicht nur auf Zeit fest. 
Denn die Dateien werden auch noch nach Größe gelöscht. 
Wenn eine Logdatei die Größe von 10 MB erreicht, wird sie gelöscht werden. 
Die Logdateien, die auf den Maschinen weiterhin gespeichert werden, sollen weiterhin nach der Dateigröße 10 MB gelöscht werden. 
Da die Daten in Zukunft in das zentralisierte System überführt werden ist es für die Dauer egal wie lange diese Dateien vorhanden sind. sollten die Daten vor den 7 Tagen gelöscht sein, dann sind die Logs immer noch im zentralisierten Server vorhanden. 
Da werden diese dann nach mindestens sieben Tagen gelöscht werden. 
Damit die Logs lange genug vorhanden sind, wird die Richtlinie auf 14 Tage gesetzt. 
So bleibt genug Puffer übrig, um zu gewährleisten das ein Fehler rechtzeitig behoben werden kann. 


% \section{Zusammenfassung der erstellten Richtlinien!}
% Hier soll einmal gesagt werden, es wurden diese Richtlinien definiert: Framework, Zentralisiertes Logging, Elastic Stack und alles andere.

% \begin{itemize}
%     \item \textbf{Framework:} NLog
%     \item \textbf{Speicherung:} Zentralisiert mit Elastic Stack
% \end{itemize}

% \section{Welche Topics sollten immer oder gar nicht geloggt werden?}


% \section{Vermeidung von Redundanzen}
% % Es sollen doppelte Log Nachrichten vermieden werden
% Anti Pattern ChenAntiPatterns

% \section{Evolution von Log Nachrichten}
% Welche Loginformationen werden oft geändert? 

\newpage
\section{Überblick}
\label{kap:uberblickRichtlinien}

Das Kapitel \ref{richtlinien} befasst sich mit den Richtlinien, die dem Team helfen sollen, die richtigen Entscheidungen beim Logging zu treffen. 
Durch diese Richtlinien wird die Effizienz beim Bugtracking verbessert. 
Folgende Richtlinien wurden in diesem Kapitel definiert: 
\begin{itemize}
    \item Ein Log muss folgende Infos enthalten:
        \begin{itemize}
            \item Timestamp
            \item LogLevel
            \item Correlation ID
            \item System Komponente 
            \item String, um den Fehler oder das Ereignis zu beschreiben
        \end{itemize}
    \item Verwendung von \textbf{strukturiertem Logging}
    \item Logging Framework: \textbf{NLog}
    \item Die Log-Level sollen so genutzt werden: 
        \begin{itemize}
            \item \textbf{try:} info oder debug
            \item \textbf{catch:} warn, error oder fatal
            \item \textbf{for und while:} info oder debug
            \item \textbf{if-else und switch:} alle Log-Level sind möglich
        \end{itemize}
    \item Speicherung der Logs zentral mit dem Elastic Stack
    \item Dauer der Speicherung: \textbf{14 Tage}
\end{itemize}

