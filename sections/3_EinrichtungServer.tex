\chapter{Einrichtung des zentralisierten Logging Servers}

Die im vorherigen Kapitel durchgeführte Evaluation kam zum entschluss, dass der Elastic Stack die Anforderungen des CFT Portale am besten erfüllen kann. 
Daher wird in diesem Kapitel ein zentralisierter Logging-Server mit dem Elastic Stack installiert und konfiguriert.
Dafür wird zu Beginn ein Server benötigt, auf dem die einzelnen Komponenten installiert werden können. 
Anschließend müssen die einzelnen Tools des Elastic Stack installiert und konfiguriert werden. \\
Dieses Kapitel ist wie folgt aufgebaut: 
\begin{itemize}
    \item RedHat Server 
    \item Elasticsearch
    \item Kibana
    \item Logstash
    \item Filebeat
\end{itemize}


\section{RedHat Server}

Damit die einzelnen Komponenten des Elastic Stack installiert werden können, musste ein Server beantragt werden. 
Dafür wurde ein Auftrag an das CFT Infrastruktur der KVWL gesendet, um so einen Server zu erhalten. 
Wie schon in Kapitel \ref{kap:bewertungTools} geschrieben wurde, werden im CFT Portale Linux Server verwendet. 
In der Regel werden RedHat Server genutzt. 
So ist es auch bei dem Server für das zentralisierte Logging. 
Es ist ein RedHat Server mit der Version 7.9. \\
Damit die Kommunikation zum Server erfolgen kann, müssen einige Ports freigeschaltet werden. 
Denn Elasticsearch und Kibana nutzen die Ports 9200 und 5601.
Ohne Freischalten der Ports würde die Kommunikation von anderen Systemen nicht Funktionieren. 
Die Portfreischaltung erfolgte mit diesen Befehlen: 
\begin{lstlisting} [language=bash]
$ sudo firewall-cmd --zone=public --add-port=9200/tcp --permanent    
$ sudo firewall-cmd --zone=public --add-port=5601/tcp --permanent    
\end{lstlisting}

Bevor die Installation der Elastic Stack Komponenten erfolgen konnte, musste festgelegt werden wie die Installation durchgeführt werden soll. 
Denn die KVWL nutzt \textit{Satellite} um die installierten Packages zu verwalten. 
Aktuell sind die Komponenten des Elastic Stack nicht im Satellite eingebunden. 
Daher muss geklärt werden wie die Komponenten installiert werden dürfen. 
Um den Organisatorischen Aufwand möglichst gering zu halten, wurde in Absprache mit den Administratoren festgelegt, dass eine Manuelle Installation derzeit durchgeführt werden soll. 
Im Anschluss an diese Bachelorarbeit sollen die Pakete, sofern das Ergebnis zufriedenstellende Ergebnisse liefert, ins Satellite eingebunden werden, um den zukünftigen Wartungsaufwand gering zu halten. 

\section{Elasticsearch}

Die erste zu installierende Komponente des Elastic Stack ist Elasticsearch.
Bei der Installation wurde auf die Anleitung vom Hersteller zurückgegriffen. \cite{elasticInstallElasticsearchRPM}
Bei der Installation war es wichtig zu beachten die richtigen Packages runterzuladen. 
Denn in der Anleitung wird von der Kommerziellen Lösung gesprochen, die kostenlos getestet werden kann. 
Die richtigen Package Versionen beinhalten ein \textit{oss} im Namen. \\ 
Aus Sicherheitsgründen hat der Server keinen Zugriff auf das Internet. 
Daher müssen die Installationspakete Manuell vom Entwickler Rechner runtergeladen werden und Anschließend zum Server rüberkopiert werden. 
Da es sich um einen RedHat Server handelt, werden \textit{rpm} Pakete runtergeladen und installiert.
Der Befehl der genutzt wurde um das Package runterzuladen ist: 
\begin{lstlisting}[language=bash]
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.10.0-x86_64.rpm
\end{lstlisting}
Nachdem der Download fertiggestellt wurde, musste das \textit{rpm} package auf den RedHat Server kopiert werden:
\begin{lstlisting} [language=bash]
pscp -P 22 elasticsearch-oss-7.10.0-x86_64.rpm bollich@DOT-RH-ZLOG1.doms.kvwl.de:/home/bollich
\end{lstlisting}
Anschließend musste das \textit{rpm} Paket installiert werden.
Nach der Installation musste eingestellt werden, dass der Elasticsearch Service automatisch startet wenn der Server hochfährt.

\begin{lstlisting} [language=bash]
sudo rpm --install elasticsearch-oss-7.10.0-x86_64.rpm
sudo /bin/systemctl daemon-reload 
sudo /bin/systemctl enable elasticsearch.service
\end{lstlisting}

Nun kann der Service gestartet werden. 
Zum Starten und Stoppen werden folgende Befehle genutzt: 
\begin{lstlisting}[language=bash]
sudo systemctl start elasticsearch.service
sudo systemctl stop elasticsearch.service    
\end{lstlisting}

Mit dem folgenden Befehl kann geprüft werden, ob der Elasticsearch Service am laufen ist: 
\begin{lstlisting}[language=bash]
curl localhost:9200
\end{lstlisting}
Die zu erwartende Ausgabe sieht wie folgt aus: 
\begin{lstlisting}[language=bash]
{
"name" : "DOT-RH-ZLOG1",
"cluster_name" : "elasticsearch",
"cluster_uuid" : "5unW2cLtQoumj2z7P75pBA",
"version" : {
"number" : "7.10.0",
"build_flavor" : "oss",
"build_type" : "rpm",
"build_hash" : "51e9d6f22758d0374a0f3f5c6e8f3a7997850f96",
"build_date" : "2020-11-09T21:30:33.964949Z",
"build_snapshot" : false,
"lucene_version" : "8.7.0",
"minimum_wire_compatibility_version" : "6.8.0",
"minimum_index_compatibility_version" : "6.0.0-beta1"
},
"tagline" : "You Know, for Search"
}
\end{lstlisting}

Hiermit ist die Installation nach der Anleitung abgeschlossen. 
Damit der Elasticsearch Service von Außerhalb erreicht werden kann, muss der Service entsprechend konfiguriert werden. 
Dafür müssen die IP-Adressen der Server eingetragen werden, die Daten an Elasticsearch senden dürfen. 
Denn nach Installation ist es nur erlaubt Daten von Localhost zu senden. 
Die Konfiguration muss in der \textit{elasticsearch.yml} Datei eingetragen werden.
Diese ist im Pfad \glqq \textit{/etc/elasticsearch/elasticsearch.yml}\grqq{} zu finden.
Für den Testfall ist es möglich zu definieren, dass von überall Daten an Elasticsearch gesendet werden können. 
Dafür muss die IP-Adresse \textit{0.0.0.0} eingetragen werden. 
Das Eintragen der IP-Adressen muss in \textit{network.host} eingetragen werden.
Wenn mehrere IP-Adressen eingetragen werden müssen, dann erfolgen die Einträge in \textit{network.hosts}. 
Elasticsearch ist mit diesen Konfigurationen nun bereit, daten zu erhalten.

\section{Kibana}

Die nächste zu installierende Komponente ist Kibana. 
Wie bei der Installation von Elasticsearch wird hier auch auf die Anleitung des Herstellers zugegriffen, um die Installation erfolgreich durchzuführen. \cite{elasticInstallKibanaRPM}
Die Installation des \textit{rpm} Packages von Kibana erfolgt vergleichbar wie die Installation des Packages von Elasticsearch. 
Daher werden die einzelnen Schritte nicht nocheinmal vorgeführt. \\

\subsection{Index Pattern}
Kibana ist im Vergleich zu Elasticsearch eine Webanwendung die von außerhalb erreicht werden kann und von Nutzern in Zukunft genutzt wird. 
Daher müssen wichtige Bestandteile der Oberfläche von Kibana näher erläutert werden. 
Zum einen müssen \textit{Index Pattern} erstellt werden, um die Logs aus dem Elasticsearch visualisieren zu können. 
Doch bevor ein \textit{Index Pattern} erstellt werden kann, müssen Daten im Elasticsearch vorhanden sein. 
Das hat den Vorteil, dass keine \textit{Index Pattern} erstellt werden können, die niemals genutzt werden können, weil keine Daten zu dem Pattern passen.
Wenn Daten vorhanden sind können im Reiter \textit{Stack Management -> Index Patterns} Index Patterns verwaltet werden.  
Dabei muss ein \textit{Index Pattern} als default definiert werden. 
Das als default definierte \textit{Index Pattern} wird beim aufrufen von Kibana angezeigt. 
Die Logs die hinter diesem Pattern stehen, werden in \textit{Discover} angezeigt.  \cite{elasticCreateIndexPattern}

\subsection{Dashboards}
Die Oberfläche von Kibana ermöglicht es individuell angepasste Dashboards zu erstellen. 
Dabei können mehrere Diagramme mit unterschiedlichen Index Pattern angezeigt werden, um die benötigten Metriken zu veranschaulichen. 
So ist es möglich schnell bestimmte Informationen anzuzeigen. 
Für die Visualisierung können alle bekannten klassischen Diagrammtypen genutzt werden. \cite{elasticKibanaVisualisierenAnalysieren}

\subsection{Kibana Query Language (KQL)}
Kibana Query Language (KQL) ist eine Abfragesprache (engl. query language), mit der das Suchen von Daten in großen Datenmengen ermöglicht wird. 
In Kibana wird KQL dazu genutzt, die vorhanden Logs nach bestimmten Kriterien zu Filtern und die gewünschten Daten zu erhalten. 
KQL setzt dabei auf eine leicht verständliche Syntax, damit keine große einarbeitung nötig ist.
Beim eintippen in der Suchleiste werden Vorschläge gegeben, um schnelles Filtern zu ermöglichen.
Jedoch wird dafür mindestens eine \textit{Basic License} benötigt.
Sollte KQL nicht gewünscht sein, kann man diese ausschalten und \textit{Lucene} nutzen.
Lucene ist sowie KQL eine Abfragesprache, die jedoch Open Source ist. 
Die Syntax von Lucene sieht so aus: \cite{apacheApacheLuceneQuery}
\begin{lstlisting}
title:"The Right Way" AND text:go
\end{lstlisting}
Um beispielhaft eine KQL Abfrage vorzuführen, wurde eine Suche in Kibana durchgeführt. 
Dabei können zwei unterschiedliche Methoden genutzt werden. 
Beide Varianten der gleich Abfrage werden hier gezeigt: 
\begin{lstlisting}
properties.LANR=8303571; properties.username=TestPraesentation
properties.LANR:8303571 or properties.username=TestPraesentation
\end{lstlisting}


\section{Logstash}

Logstash ist eine der Komponenten aus dem Elastic Stack. 
Dabei ist Logstash eines der mächtigsten Tools aus dem Elastic Stack.
Mit Logstash können die Logs Manipuliert werden und noch vieles mehr. 
Jedoch ist ein großes Problem von Logstash, dass es sehr viel Leistung benötigt betrieben zu werden und sehr viel Wissen um es richtig zu konfigurieren. 
Daher kann Logstash Probleme mit sich bringen. 
Im laufe der Durchführung ist dabei aufgefallen, dass Logstash im Falle des CFT Portale nicht benötigt wird. 
Denn Filebeat übernimmt das Senden der Logs an Elasticsearch und ist dabei leichter zu konfigurieren und benötigt nicht viel Leistung. 

\section{Filebeat}
Filebeat ist die letzte zu installierende Komponente des Elastic Stack. 
Die Installation von Filebeat muss etwas anders ablaufen. 
Denn Filebeat wird auf den Windows Servern der produktiven und test Systemen installiert. 
Daher muss eine Windows Installation durchgeführt werden. 
Zu Beginn muss ein Zip File runtergeladen werden, dass die Open Source Version von Filebeat enthält. 
Nach der Extraktion des Archivs, muss ein PowerShell-Skript als Administrator ausgeführt werden. 
Das PowerShell-Skript \textit{install-service-filebeat.ps1} führt die komplette installation durch. \\
Nachdem die Installation abgeschlossen ist, muss die Verbindung zu Elasticsearch und Kibana hergestellt werden. 
Dafür muss die Konfigurationsdatei \textit{Filebeat.yml} angepasst werden. 
Zu erst muss eine Verbindung zum Elasticsearch hergestellt werden. 
Das funktioniert mit folgender Zeile in der Konfigurationsdatei:
\begin{lstlisting}
output.elasticsearch:
    hosts: ["http://DOT-RH-ZLOG1.doms.kvwl.de:9200"]
\end{lstlisting}
Sollte es mehrere Elasticsearch Services geben, können diese in das Interval der Hosts mit eingetragen werden. 
So kann bei hoher Last zu mehreren Services gesendet wendet. \\
Damit vordefinierte Index Pattern an Kibana gesendet werden können, muss Kibana auch in der Konfigurationsdatei eingetragen werden.
Der Befehl ist dabei ein bisschen anders: 
\begin{lstlisting}
setup.kibana:
    host: "http://DOT-RH-ZLOG1.doms.kvwl.de:5601"
\end{lstlisting}
Damit der Filebeat Service weiß welche Logs zu Elasticsearch gesendet werden sollen, müssen noch \textit{inputs} definiert werden.
Dabei können mehrere Inputs definiert werden. 
Die Inputs werden dabei getrennt mit \glqq -\grqq{} angegeben. 
Für die Inputs können verschiedene Konfigurationen definiert werden. 
Damit die Logs der Vierteljahreserklärung gesammelt und gesendet werden, wird diese Konfiguration benötigt: 
\begin{lstlisting}
filebeat.inputs:
- type: log
  enabled: true
  paths: D:\logs\KVWL.PortalVEAddIn\*
  json.keys_under_root: true
  json.add_error_key: true
  fields:
    app_name: VEAddIn
\end{lstlisting}
Zeile 1 definiert, dass jetzt \textit{inputs} folgen. 
Zeile 2 bis 8 sind die Konfigurationen von dem einen \textit{input}.
Zeile 5 und 6 sind dabei wichtig, denn diese ermöglichen das Einlesen von strukturieren Log-Daten im JSON-Format. 
Zeile 7 und 8 beschreiben selbst definierte Felder in den gesendeten Logs. 
Da können Informationen für alle Logs aus diesem Input mitgesendet werden. 
In diesem Fall ist das der Name der Anwendung der mitgeschickt wird. 
So kann in Kibana nach den einzelnen Apps gefiltert werden. 

\section{Struktur der Logs}

Das CFT Portale arbeitet mit 3 Stages: Prod, Test und Dev.
Dabei beschreibt Prod das produktive System auf denen die Apps für die Mitglieder installiert werden. 
Test ist das Testsystem auf denen die Software getestet wird bevor sie ins produktive System deployed wird. 
Dabei ist das Testsystem identisch zum produktiven System aufgebaut.
So können Konfigurationsfehler ausgeschlossen werden. 
Dev spiegelt eine Umgebung wieder, in der das Team neue Features testen kann. 
Die Dev Umgebung kann sich von der Test- und der Prod Umgebung unterscheiden. \\
Während in der Dev und Test Umgebung nur ein Server läuft, wird in der Prod Umgebung auf zwei Server gesetzt die gespiegelt sind. 
Die zwei gespiegelten Server sollen dafür sorgen, dass ausfälle abgefangen werden können. \\

Die Abbildung \ref{fig:stages} zeigt die Serverstruktur des CFT Portale.
Dabei kann man alle Server erkennen, auf denen die Provider hosted Apps des CFT Portale installiert sind und zusätzlich des zentralisierten Log-Server.
Der zentralisierte Log-Server hat den Namen: \textit{DOT-RH-ZLOG1}.
Die anderen Server sind je nach Stage benannt. 
Dabei ist die erste Stage Prod. 
Auf Prod sind zwei Server vorhanden, die gespiegelt sind. 
Dort sind die Apps installiert in der produktiven Umgebung.
Die Servernamen sind: \textit{PHA1} und \textit{PHA2}.
Für Test und Dev gibt es jeweils einen Server mit dem gleichen Namen wie die Stage. \\

Auf jedem der in der Abbildung \ref{fig:stages} zu sehenden Server der drei Stages, sind alle Provider hosted Apps des CFT Portale installiert.
Das heißt, dass für einen Server jeweils ein Filebeat Agent installiert wurde. 
Dieser Agent kümmert sich um die Logs der Apps auf diesem einen Server. 
Das bedeutet, dass jede App vier mal installiert ist und auch genau so viele Logs von unterschiedlichen Servern schreiben wird. 
Damit die Logs lesbar und strukturiert bleiben, musste eine durchsuchbare Lösung erarbeitet werden. \\ 

\begin{figure}[H]
    \center 
    \includegraphics[width=1\textwidth]{pictures/StagesMitLogs.png} 
    \caption{Stages mit Logs }
    \label{fig:stages}
\end{figure}

Damit die Suche in Kibana effektiv durchgeführt werden kann, werden drei Index Pattern erstellt. 
Jeweils für eine Stage. 
Also sind die drei Patterns dann: \textit{Prod}, \textit{Test} und \text{Dev}.
So kann eine erste Schnelle Trennung der zuständigkeiten erreicht werden. 
Da natürlich pro Stage immer noch eine hohe Anzahl an unterschiedlichen Apps vorhanden ist, wird es schwierig darin die erforderlichen Daten zu finden. 
Die Patterns sollen als erste Filterung genutzt werden um den Anwendungsfall zu finden. 
Anschließend soll nach den Apps gefiltert werden können. 
Damit diese geschehen kann, muss in der Filebeat Konfiguration für jedes Input ein extra Feld definiert werden in dem der Appname definiert ist. 
Die extra Zeilen für ein Input sehen so aus: 
\begin{lstlisting}
fields:
    app_name: VEAddIn
\end{lstlisting}
Das Beispiel zeigt die Konfigurationen für die Vierteljahreserklärung.
Mit diesen zusätzlichen Feldern ist eine effektive Filterung nach Stages und Anschließend nach den Apps möglich. 
Für das Team ist ein häufig auftretender Fall in der produktiven Umgebung, dass es notwendig ist zu wissen auf welchem der beiden produktiven Server (PHA1 und PHA2) die Logs geschrieben wurden. 
Filebeat sendet dafür automatisch die Informationen über den Hostname mit. 
Somit muss das Team zusätzlich zu den zwei Filterungsoptionen noch nach dem \textit{agent.hostname} Filtern.


